{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Multi-View Image Classification</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T19:39:24.725471Z",
     "start_time": "2019-12-23T19:39:22.659395Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#=============================================================================================================#\n",
    "#                                                   IMPORTS                                                   #\n",
    "#=============================================================================================================#\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import csv\n",
    "import copy\n",
    "\n",
    "from collections import OrderedDict\n",
    "from scipy import spatial\n",
    "import glob\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV, StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "from torchvision import models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "import pickle\n",
    "from joblib import dump, load\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-21T22:42:45.866775Z",
     "start_time": "2019-12-21T22:42:45.853411Z"
    }
   },
   "outputs": [],
   "source": [
    "#======================================================================================================================#\n",
    "#                                           UTILITY FUNCTIONS & GLOBALS                                                #\n",
    "#======================================================================================================================#\n",
    "\n",
    "def compute_sample_weight(class_weights, y):\n",
    "    \"\"\"\n",
    "    Generate a weight array for the array y\n",
    "\n",
    "    :param class_weights: (type dict)\n",
    "        weights of the classes in y\n",
    "    :param y: (type iterable)\n",
    "        array for which the weight array has to be generated\n",
    "    :return: (type numpy array)\n",
    "        weight array\n",
    "    \"\"\"\n",
    "\n",
    "    assert isinstance(class_weights, dict)\n",
    "    result = np.array([class_weights[i] for i in y])\n",
    "    return result\n",
    "\n",
    "# Class Weights used in the weighted accuracy\n",
    "class_weights = {\n",
    "    0: 0.327,\n",
    "    2: 2.0,\n",
    "    3: 2.572,\n",
    "    1: 0.586,\n",
    "    5: 15.0,\n",
    "    4: 26.452}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-21T14:03:13.448479Z",
     "start_time": "2019-12-21T14:03:13.444681Z"
    }
   },
   "outputs": [],
   "source": [
    "# GLOBALS\n",
    "\n",
    "# Data Directory\n",
    "data_dir = '../data/raw'\n",
    "\n",
    "# Image Processing Parameters \n",
    "BLUR = 21\n",
    "CANNY_THRESH_1 = 10\n",
    "CANNY_THRESH_2 = 200\n",
    "MASK_DILATE_ITER = 10\n",
    "MASK_ERODE_ITER = 10\n",
    "MASK_COLOR = (0.0, 0.0, 1.0) # In BGR format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-21T14:03:14.046533Z",
     "start_time": "2019-12-21T14:03:14.037082Z"
    }
   },
   "outputs": [],
   "source": [
    "# IMAGE PREPROCESSING FUNCTION\n",
    "\n",
    "def preprocess(filename):\n",
    "    img = cv2.imread(filename)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    #-- Edge detection -------------------------------------------------------------------\n",
    "    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)\n",
    "    edges = cv2.dilate(edges, None)\n",
    "    edges = cv2.erode(edges, None)\n",
    "    \n",
    "    #-- Find contours in edges, sort by area ---------------------------------------------\n",
    "    contour_info = []\n",
    "    _, contours, _ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)\n",
    "\n",
    "    for c in contours:\n",
    "        contour_info.append((\n",
    "            c,\n",
    "            cv2.isContourConvex(c),\n",
    "            cv2.contourArea(c),\n",
    "        ))\n",
    "    contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)\n",
    "    max_contour = contour_info[0]\n",
    "    \n",
    "    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----\n",
    "    # Mask is black, polygon is white\n",
    "    mask = np.zeros(edges.shape)\n",
    "    cv2.fillConvexPoly(mask, max_contour[0], (255))\n",
    "\n",
    "    #-- Smooth mask, then blur it --------------------------------------------------------\n",
    "    mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)\n",
    "    mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)\n",
    "    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)\n",
    "\n",
    "    #-- Create final image ---------------------------------------------------------------\n",
    "    img[mask <= 100] = 0\n",
    "    return img\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-21T14:03:15.069358Z",
     "start_time": "2019-12-21T14:03:14.716781Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Preprocessing an example image\n",
    "filename = '../data/raw/original_2.png'\n",
    "img = preprocess(filename)\n",
    "\n",
    "plt.imshow(img);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-21T14:25:56.275161Z",
     "start_time": "2019-12-21T14:25:55.313176Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Preprocess all images and Save them to a new directory `processed`\n",
    "\n",
    "filenames = glob.glob(os.path.join(data_dir, '*.png')) # list all image file names in the data folder\n",
    "proc_data_dir = '../data/processed'\n",
    "\n",
    "for fname in filenames:\n",
    "    img_name = os.path.basename(fname)\n",
    "    img = preprocess(fname)\n",
    "    cv2.imwrite(os.path.join(proc_data_dir, os.path.splitext(img_name)[0] + '_processed.png'), img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-22T18:51:06.730866Z",
     "start_time": "2019-12-22T18:51:06.723146Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function for visualizing an image\n",
    "\n",
    "def show_plug(plug_name, data_dir, preprocess=None):\n",
    "    # Read Images\n",
    "    _fnames = glob.glob(os.path.join(data_dir, f'{_name}_*.png'))\n",
    "    if preprocess:\n",
    "        _images = [preprocess(_fname) for _fname in _fnames]\n",
    "    else:\n",
    "        _images = [cv2.cvtColor(cv2.imread(_fname), cv2.COLOR_BGR2RGB) for _fname in _fnames]\n",
    "    # Visualize Images\n",
    "    plt.figure(figsize=(18,8))\n",
    "    for i in range(len(_images)):\n",
    "        plt.subplot(2, 4, i+1)\n",
    "        plt.imshow(_images[i])\n",
    "        plt.title(os.path.basename(_fnames[i]))\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        \n",
    "    plt.suptitle(f'IMAGE: {_name}', size=20)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-22T18:51:09.908542Z",
     "start_time": "2019-12-22T18:51:07.050292Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Example Visualization\n",
    "\n",
    "_name = 'original_2'\n",
    "show_plug(plug_name, data_dir)\n",
    "show_plug(plug_name, data_dir, preprocess=preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Feature Extraction & Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Build Vocabulary of Visual Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is building a vocabulary of visual words. This task relies on one or a combination of multiple traditional computer vision feature extractors such as : HoG, SURF, SIFT etc. Let us first use SURF to extract features from an example image, visualize the key-points identified, then extract features from all of the images into a matrix of descriptors. We then use K-means on this matrix to come up with our K visual words (represented by the K centroids)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T17:21:55.130830Z",
     "start_time": "2019-12-24T17:21:54.895243Z"
    }
   },
   "outputs": [],
   "source": [
    "# Detect and Visualize Keypoints in an Image\n",
    "\n",
    "# Choosing a high value for the hessian threshold in order to visualize only a few key-points to avoid clutter\n",
    "surf = cv2.xfeatures2d.SURF_create(2000) \n",
    "\n",
    "img = cv2.imread('../data/processed/original_2_13_processed.png')\n",
    "img = cv2.resize(cv2.cvtColor(img, cv2.COLOR_BGR2GRAY), (900, 900))\n",
    "\n",
    "kp, descriptors = surf.detectAndCompute(img, None)\n",
    "print('The number of key points detected:', len(kp))\n",
    "\n",
    "img_kp = cv2.drawKeypoints(img, kp, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(img_kp, cmap='gray')\n",
    "plt.xticks([])\n",
    "plt.yticks([]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-21T20:53:17.847968Z",
     "start_time": "2019-12-21T20:51:07.140254Z"
    }
   },
   "outputs": [],
   "source": [
    "# CREATE THE MATRIX OF DESCRIPTORS FROM ALL IMAGES\n",
    "\n",
    "proc_data_dir = '../data/processed'\n",
    "filenames = glob.glob(os.path.join(proc_data_dir, '*.png')) # list all image file names in the data folder\n",
    "\n",
    "surf = cv2.xfeatures2d.SURF_create(300) # create SURF feature extractor\n",
    "\n",
    "\n",
    "counter = 0\n",
    "list_descriptors = []\n",
    "\n",
    "for filename in filenames:\n",
    "    img = cv2.imread(filename)\n",
    "    img = cv2.resize(cv2.cvtColor(img, cv2.COLOR_BGR2GRAY), (300, 300))\n",
    "    kp, descriptors = surf.detectAndCompute(img, None)\n",
    "    if descriptors is not None:\n",
    "        list_descriptors.append(descriptors)\n",
    "\n",
    "# This is a matrix where each row is a feature vector extracted from an image (possible to have multiple features per image)      \n",
    "# And each column is a dimension of the SURF feature vector (total 64, can also be 128 which can be set in the definition of surf) \n",
    "descriptor_matrix = np.vstack(list_descriptors)\n",
    "print('Shape of the Matrix of Descriptors', descriptor_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-21T21:27:40.146585Z",
     "start_time": "2019-12-21T20:54:09.341966Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# CLUSTER FEATURE VECTORS TO CONSTRUCT THE CODEBOOK (i.e VOCABULARY OF VISUAL WORDS)\n",
    "# Training Kmeans takes time, we save it at the end so we can reload it again later\n",
    "# Note : bigger K (eg. 500 or 800) will yield better results at the expense of more computation time\n",
    "K = 200\n",
    "km = KMeans(n_clusters=K, n_jobs=-1)\n",
    "km.fit(descriptor_matrix)\n",
    "centroids = km.cluster_centers_\n",
    "print('The shape of the centroids:', centroids.shape)\n",
    "\n",
    "del descriptor_matrix # To free up memory\n",
    "\n",
    "# Save the k-means model and its centroids\n",
    "#np.save('../resources/vocabulary_surf.npy', centroids)\n",
    "#pickle.dump(km, open('../models/kmeans.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Build Matrix of Feature Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we build the matrix of feature vectors by transforming each image into a vector of size K (number of clusters from before). For each we process all its image views, extract their SURF descriptors, find the cluster of each descriptor (i.e. the visual word associated with the descriptor), then count the number of times each visual word (i.e. cluster) occurs. We end up with a feature vector of size K for that Cimage. Stacking these feature vectors vertically results in a NxK matrix where each row represents an image and each column, a visual word and each cell, the number of times the visual word occurs in the image (i.e all of its image views)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T19:44:27.664649Z",
     "start_time": "2019-12-23T19:44:27.651414Z"
    }
   },
   "outputs": [],
   "source": [
    "# FUNCTION DEFINITION : turns an image set into a feature vector of size K through image quantization.\n",
    "# We use the K-means model trained on the matrix of descriptors\n",
    "# Iterate through the images, process each image, create its SURF feature vectors\n",
    "# Predict the cluster of each feature vector, add 1 to the output vector at the location of the predicted cluster (Quantization)\n",
    "\n",
    "def get_feature_vec(plug_name, model, preprocess=None, surf_threshold=200):\n",
    "    \n",
    "    feature_vec = np.zeros(K) # Intialize vector representation of visual words\n",
    "    \n",
    "    surf = cv2.xfeatures2d.SURF_create(surf_threshold)\n",
    "    \n",
    "    _fnames = glob.glob(os.path.join(proc_data_dir, f'{_name}_*.png'))\n",
    "    if preprocess:\n",
    "        _images = [cv2.resize(cv2.cvtColor(preprocess(_fname), cv2.COLOR_BGR2GRAY), (300, 300)) for plug_fname in _fnames] \n",
    "    else:\n",
    "        _images = [cv2.resize(cv2.cvtColor(cv2.imread(_fname), cv2.COLOR_BGR2GRAY), (300, 300)) for plug_fname in _fnames] \n",
    "    \n",
    "    for image in _images: # iterate over the image views of the image\n",
    "        kp, descriptors = surf.detectAndCompute(image, None) # extract the surf descriptors of the image\n",
    "        if descriptors is not None:\n",
    "            visual_words = model.predict(descriptors) # find all the visual words in the image\n",
    "            for visual_word in visual_words: # find the occurence of each visual word\n",
    "                feature_vec[visual_word] += 1\n",
    "\n",
    "    return feature_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T19:51:46.918818Z",
     "start_time": "2019-12-23T19:51:44.849076Z"
    }
   },
   "outputs": [],
   "source": [
    "# Example : Visualizing the feature vector of two images as a bar graph\n",
    "\n",
    "_name_1 = 'original_2'\n",
    "_name_2 = 'original_3'\n",
    "\n",
    "_fname_1 = os.path.join(proc_data_dir, f'{plug_name_1}_1_processed.png')\n",
    "_fname_2 = os.path.join(proc_data_dir, f'{plug_name_2}_1_processed.png')\n",
    "\n",
    "img_1 = cv2.cvtColor(cv2.imread(plug_fname_1), cv2.COLOR_BGR2RGB)\n",
    "img_2 = cv2.cvtColor(cv2.imread(plug_fname_2), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "\n",
    "kp_1 = surf.detect(img_1)\n",
    "kp_2 = surf.detect(img_2)\n",
    "\n",
    "plt.figure(figsize=(18,10))\n",
    "\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.imshow(img_1)\n",
    "plt.title('Image view 1')\n",
    "plt.xticks([]); plt.yticks([]) \n",
    "plt.subplot(2, 3, 2)\n",
    "plt.imshow(cv2.drawKeypoints(cv2.cvtColor(img_1, cv2.COLOR_RGB2GRAY), kp_1, None, flags=cv2.DrawMatchesFlags_DRAW_RICH_KEYPOINTS))\n",
    "plt.title('Keypoints detected')\n",
    "plt.xticks([]); plt.yticks([]) \n",
    "plt.subplot(2, 3, 3)\n",
    "plt.bar(np.arange(1, K+1), get_feature_vec(_name_1, km), color='mediumaquamarine', width=0.3)\n",
    "plt.title('Feature Vector')\n",
    "\n",
    "plt.subplot(2, 3, 4)\n",
    "plt.imshow(img_2)\n",
    "plt.title('Image view 2')\n",
    "plt.xticks([]); plt.yticks([]) \n",
    "plt.subplot(2, 3, 5)\n",
    "plt.imshow(cv2.drawKeypoints(cv2.cvtColor(img_2, cv2.COLOR_RGB2GRAY), kp_2, None, flags=cv2.DrawMatchesFlags_DRAW_RICH_KEYPOINTS))\n",
    "plt.title('Keypoints detected')\n",
    "plt.xticks([]); plt.yticks([]) \n",
    "plt.subplot(2, 3, 6)\n",
    "plt.bar(np.arange(1, K+1), get_feature_vec(_name_2, km), color='mediumaquamarine', width=0.3)\n",
    "plt.title('Feature Vector of images');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-21T22:40:27.854614Z",
     "start_time": "2019-12-21T22:38:12.727431Z"
    }
   },
   "outputs": [],
   "source": [
    "# BUILD THE MATRIX OF FEATURE VECTORS (NxK)\n",
    "\n",
    "_names = np.unique([fname.split('_')[0] for fname in os.listdir(proc_data_dir) if fname.endswith('.png')])\n",
    "N = len(_names)\n",
    "print(f'There are {N} images.')\n",
    "\n",
    "X = np.empty((N, K))\n",
    "\n",
    "for i, _name in enumerate(_names):\n",
    "    X[i] = get_feature_vec(_name, km)\n",
    "\n",
    "print('Shape of the feature matrix', X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-21T22:25:40.549819Z",
     "start_time": "2019-12-21T22:25:40.529570Z"
    }
   },
   "source": [
    "### 3. Train a Machine Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-21T23:29:13.573159Z",
     "start_time": "2019-12-21T23:29:13.055417Z"
    }
   },
   "outputs": [],
   "source": [
    "# GET CAR PLUG LABELS\n",
    "df_class_map = pd.read_csv('../data/processed/train.csv')\n",
    "df_class_map.head()\n",
    "\n",
    "labels = [df_class_map.loc[df_class_map.part_no == _name, 'label'].item() for _name in _names]\n",
    "labels[:10]\n",
    "\n",
    "y = np.array([class_id_map[label] for label in labels]) # Encode the string labels into code numbers\n",
    "y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-21T23:33:56.682603Z",
     "start_time": "2019-12-21T23:33:56.676774Z"
    }
   },
   "outputs": [],
   "source": [
    "# TRAIN-TEST SPLIT\n",
    "rnd = 42\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=rnd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-21T23:33:57.277220Z",
     "start_time": "2019-12-21T23:33:57.272445Z"
    }
   },
   "outputs": [],
   "source": [
    "# NORMALIZE DATA\n",
    "m = X_train.mean(axis=0)\n",
    "std = X_train.std(axis=0)\n",
    "\n",
    "X_train = (X_train - m)/std\n",
    "X_test = (X_test - m)/std\n",
    "\n",
    "# Need to save mean and std of training data, we use these values to normalize test data and new data as well\n",
    "#np.save('../resources/scaler_mean', m)\n",
    "#np.save('../resources/scaler_std', std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-21T23:33:59.417900Z",
     "start_time": "2019-12-21T23:33:59.221944Z"
    }
   },
   "outputs": [],
   "source": [
    "# TRAIN A LOGISTIC REGRESSION MODEL \n",
    "# We using cost-sensitive learning by providing the class weights to steer the learning towards making less costly mistakes as represented by the weights\n",
    "\n",
    "lr = LogisticRegression(class_weight=class_weights)\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "y_pred = lr.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, \n",
    "                          y_pred, \n",
    "                          sample_weight=compute_sample_weight(class_weights, y_test))\n",
    "\n",
    "print(f'The accuracy of Logistic Regression is {round(accuracy*100, 2)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-21T23:46:11.985575Z",
     "start_time": "2019-12-21T23:46:11.982681Z"
    }
   },
   "outputs": [],
   "source": [
    "# SAVE THE MODEL\n",
    "#pickle.dump(lr, open('../models/logistic_regression.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment :**\n",
    "\n",
    "Although the accuracy seems high, it has a very large variance. If we resample the training and test set again, we may obtain a very different performance value. The values typically range between 76% and 94% with an average around 87%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-22T00:04:43.476042Z",
     "start_time": "2019-12-22T00:04:43.471050Z"
    }
   },
   "outputs": [],
   "source": [
    "# FUNCTION TO GET PREDICTIONS FOR IMAGES\n",
    "\n",
    "def lr_pred(_name, data_dir, pred_model, clus_model, scaler_mean, scaler_std):\n",
    "\n",
    "    feature_vec = get_feature_vec(_name, clus_model, preprocess=preprocess)\n",
    "    feature_vec = (feature_vec - scaler_mean) / scaler_std\n",
    "    pred = pred_model.predict(feature_vec.reshape(1, -1))[0]\n",
    "    return pred, {v:k for k,v in class_id_map.items()}[pred]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-22T00:04:33.919166Z",
     "start_time": "2019-12-22T00:04:33.212537Z"
    }
   },
   "outputs": [],
   "source": [
    "# GET NEW PREDICTIONS FROM RAW IMAGES\n",
    "_name = 'original_6'\n",
    "lr_pred(_name, data_dir, lr, km, m, std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Multi-View Convolutional Neural Network (MVCNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-22T00:17:27.251798Z",
     "start_time": "2019-12-22T00:17:27.245036Z"
    }
   },
   "outputs": [],
   "source": [
    "# CREATE STRATIFIED TRAIN-VALIDATION SPLIT INDICES\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2)\n",
    "y = [dataset._label_map[image] for image in dataset._names]\n",
    "\n",
    "train_indices, val_indices = next(sss.split(np.zeros(len(y)), y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-22T00:18:29.557001Z",
     "start_time": "2019-12-22T00:18:29.552523Z"
    }
   },
   "outputs": [],
   "source": [
    "# CREATE TRAIN AND VALIDATION DATA LOADERS\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "val_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=32, sampler=train_sampler, num_workers=8)\n",
    "val_loader = DataLoader(dataset, batch_size=32, sampler=val_sampler, num_workers=8)\n",
    "data_loaders = {'train': train_loader, 'val': val_loader}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-22T00:15:40.148001Z",
     "start_time": "2019-12-22T00:15:40.138423Z"
    }
   },
   "source": [
    "## 2. Create the Multi-View CNN (MVCNN) Architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-22T00:20:15.751857Z",
     "start_time": "2019-12-22T00:20:15.744602Z"
    }
   },
   "outputs": [],
   "source": [
    "# MULTI-VIEW CONVOLUTIONAL NEURAL NETWORK (MVCNN) ARCHITECTURE\n",
    "\n",
    "class MVCNN(nn.Module):\n",
    "    def __init__(self, num_classes=1000, pretrained=True):\n",
    "        super(MVCNN, self).__init__()\n",
    "        resnet = models.resnet34(pretrained = pretrained)\n",
    "        fc_in_features = resnet.fc.in_features\n",
    "        self.features = nn.Sequential(*list(resnet.children())[:-1])\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(fc_in_features, 2048),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(2048, 2048),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(2048, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs): # inputs.shape = samples x views x height x width x channels\n",
    "        inputs = inputs.transpose(0, 1)\n",
    "        view_features = [] \n",
    "        for view_batch in inputs:\n",
    "            view_batch = self.features(view_batch)\n",
    "            view_batch = view_batch.view(view_batch.shape[0], view_batch.shape[1:].numel())\n",
    "            view_features.append(view_batch)   \n",
    "            \n",
    "        pooled_views, _ = torch.max(torch.stack(view_features), 0)\n",
    "        outputs = self.classifier(pooled_views)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-22T00:20:40.447768Z",
     "start_time": "2019-12-22T00:20:30.823270Z"
    }
   },
   "outputs": [],
   "source": [
    "# BUILD AND VISUALIZE THE MODEL\n",
    "model = MVCNN(num_classes=6, pretrained=True)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train the MVCNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-22T00:27:13.619538Z",
     "start_time": "2019-12-22T00:27:13.614925Z"
    }
   },
   "outputs": [],
   "source": [
    "# DEFINE THE DEVICE\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-22T00:27:14.238124Z",
     "start_time": "2019-12-22T00:27:14.218478Z"
    }
   },
   "outputs": [],
   "source": [
    "# DEFINE A FUNCTION TO TRAIN THE MODEL\n",
    "\n",
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            all_preds = []\n",
    "            all_labels = []\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    # Get model predictions\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                all_preds.append(preds)\n",
    "                all_labels.append(labels)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].sampler.indices)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].sampler.indices)\n",
    "            all_labels = torch.cat(all_labels, 0)\n",
    "            all_preds = torch.cat(all_preds, 0)\n",
    "            epoch_weighted_acc = accuracy_score(all_labels.cpu().numpy(), all_preds.cpu().numpy(), sample_weight=compute_sample_weight(class_weights, all_labels.cpu().numpy()))\n",
    "            \n",
    "\n",
    "            print('{} Loss: {:.4f} - Acc: {:.4f} - Weighted Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc, epoch_weighted_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_weighted_acc > best_acc:\n",
    "                best_acc = epoch_weighted_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_weighted_acc)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Training the Classifier Block of the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FREEZE THE WEIGHTS IN THE FEATURE EXTRACTION BLOCK OF THE NETWORK (I.E. RESNET BASE)\n",
    "for param in model.features.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN THE CLASSIFIER BLOCK OF THE MODEL (I.E TOP DENSE LAYERS)\n",
    "model.to(device)\n",
    "EPOCHS = 40\n",
    "weight = torch.tensor([0.327, 0.586, 2.0, 2.572, 26.452, 15.0]).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=weight)\n",
    "optimizer = optim.Adam(model.classifier.parameters(), lr=0.0005)\n",
    "\n",
    "model, val_acc_history = train_model(model=model, dataloaders=data_loaders, criterion=criterion, optimizer=optimizer, num_epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-22T00:24:47.039354Z",
     "start_time": "2019-12-22T00:24:47.034122Z"
    }
   },
   "outputs": [],
   "source": [
    "# SAVE CURRENT WEIGHTS OF THE MODEL (STAGE 1: FEATURE EXTRACTION)\n",
    "torch.save(model.state_dict(), '../models/mvcnn_stage_1.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Fine-Tuning the Entire Network (Feature Extractor + Classifier Blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNFREEZE ALL THE WEIGHTS OF THE NETWORK\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# FINE-TUNE THE ENTIRE MODEL (I.E FEATURE EXTRACTOR + CLASSIFIER BLOCKS) USING A VERY SMALL LEARNING RATE\n",
    "EPOCHS = 20\n",
    "criterion = nn.CrossEntropyLoss(weight=weight)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00005) # We use a smaller learning rate\n",
    "\n",
    "model, val_acc_history = train_model(model=model, dataloaders=data_loaders, criterion=criterion, optimizer=optimizer, num_epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE CURRENT WEIGHTS OF THE MODEL (STAGE 2: FINE-TUNING)\n",
    "torch.save(model.state_dict(), '../models/mvcnn_stage_2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION TO GET PREDICTIONS FOR NEW IMAGES\n",
    "def mvcnn_pred(_name, data_dir, model, device):\n",
    "    transform = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor()])\n",
    "    _fnames = glob.glob(data_dir + f'/{_name}_*.png')\n",
    "    image = torch.stack([transform(Image.open(fname).convert('RGB')) for fname in _fnames]).unsqueeze(0)\n",
    "    image = image.to(device)\n",
    "    pred = torch.nn.functional.softmax(model(image)).argmax().item()\n",
    "    return pred, {v:k for k,v in class_id_map.items()}[pred]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET NEW PREDICTIONS FROM RAW IMAGES\n",
    "plug_name = 'original_12'\n",
    "mvcnn_pred(_name, '../data/processed', model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
